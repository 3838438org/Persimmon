Evaluation
==========

From the conception of the project it was intended to be tested by
participants, evaluating the usability of the prototype.
This chapter explains that evaluation process, how the survey was designed, and
the analysis of the results.


Method
------
The method for conducting the evaluation is mainly based on conducting a series
of closed tasks and giving feedback on each of them.

The questionnaire selected is the Single Ease Question [@sauro201210], that
ask how difficult the task was on a seven point scale.
Along that information the participant knowledge and familiarity with Data
Mining and Machine Learning is saved, as well as any additional feedback about
the system.


Proposed tasks
--------------
The evaluation is composed by three different closed tasks.
The task are defined as to gradually introduce more complex concepts, following
the seen workflows on the workflow chapter and also being introduced to the
concepts of reconnecting and complex block that require more actions than just
connecting.

* First task is the creation of a simple workflow, the objective of
    this task being to introduce Persimmon to the participants in the simplest
    terms. Using the iris dataset they perform a cross validation evaluation
    of their chosen classificator.
* Second task is modifying the previous workflow to create a more complex
    worflow thatfits and predicts using an estimators and two sources of files.
    It is only slightly more complex than the previous one, but it introduces
    the concept of re-cabling to the participants.
* Third task and final task. This one involves adding hyper-parameter tunning,
    which in turns means providing a dictionary with desired parameters.


Evaluation Results
------------------
<!-- Actual evaluation -->
